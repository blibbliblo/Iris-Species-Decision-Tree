{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMKdBNVNU5VDg7nUfVoh0r1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/blibbliblo/Iris-Species-Decision-Tree/blob/main/IrisSpecies_DecisionTree.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "VGxrw6cxsW7u"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"uciml/iris\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rkqYyxdxygZG",
        "outputId": "5c388ad4-966e-4bbb-d8da-c6a479cd34be"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Colab cache for faster access to the 'iris' dataset.\n",
            "Path to dataset files: /kaggle/input/iris\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv(f\"{path}/Iris.csv\")\n",
        "\n",
        "X = data.drop(columns=[\"Id\", \"Species\"]).values\n",
        "y = data[\"Species\"].values\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)"
      ],
      "metadata": {
        "id": "M0OBIvpywCYM"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Node:\n",
        "    def __init__(self, feature_idx=None, threshold=None, info_gain=None, left=None, right=None, value=None):\n",
        "\n",
        "        # decision node\n",
        "        self.feature_idx = feature_idx\n",
        "        self.threshold = threshold\n",
        "        self.info_gain = info_gain\n",
        "        self.left = left\n",
        "        self.right = right\n",
        "\n",
        "        # leaf node\n",
        "        self.value = value"
      ],
      "metadata": {
        "id": "gto0eH0cwJyX"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DecisionTree:\n",
        "    def __init__(self, min_samples_split=2, max_depth=2):\n",
        "\n",
        "        self.min_samples_split = min_samples_split\n",
        "        self.max_depth = max_depth\n",
        "\n",
        "    def build_tree(self, dataset, curr_depth=0):\n",
        "        X, y = dataset[:, :-1], dataset[:, -1]\n",
        "        n_samples, n_features = X.shape\n",
        "\n",
        "        if n_samples >= self.min_samples_split and curr_depth <= self.max_depth:\n",
        "            best_split = self.best_split(dataset, n_features)\n",
        "\n",
        "            if best_split[\"info_gain\"] > 0:\n",
        "                left_node = self.build_tree(best_split[\"left_dataset\"], curr_depth + 1)\n",
        "                right_node = self.build_tree(best_split[\"right_dataset\"], curr_depth + 1)\n",
        "\n",
        "                return Node(best_split[\"feature_idx\"], best_split[\"threshold\"], best_split[\"info_gain\"], left_node, right_node)\n",
        "\n",
        "        leaf_value = Counter(y).most_common(1)[0][0]\n",
        "        return Node(value=leaf_value)\n",
        "\n",
        "    def best_split(self, dataset, n_features):\n",
        "        best_split = {'feature_idx': None, 'threshold': None, 'info_gain': -1, 'left_dataset': None, 'right_dataset': None}\n",
        "\n",
        "        for feature_idx in range(n_features):\n",
        "            feature_values = dataset[:, feature_idx]\n",
        "            thresholds = np.unique(feature_values)\n",
        "\n",
        "            for threshold in thresholds:\n",
        "                left_dataset, right_dataset = self.split(dataset, feature_idx, threshold)\n",
        "\n",
        "                if len(left_dataset) and len(right_dataset):\n",
        "                    parent_y, left_y, right_y = dataset[:, -1], left_dataset[:, -1], right_dataset[:, -1]\n",
        "\n",
        "                    info_gain = self.information_gain(parent_y, left_y, right_y)\n",
        "\n",
        "                    if info_gain > best_split['info_gain']:\n",
        "                        best_split['feature_idx'] = feature_idx\n",
        "                        best_split['threshold'] = threshold\n",
        "                        best_split['info_gain'] = info_gain\n",
        "                        best_split['left_dataset'] = left_dataset\n",
        "                        best_split['right_dataset'] = right_dataset\n",
        "\n",
        "        return best_split\n",
        "\n",
        "    def split(self, dataset, feature_idx, threshold):\n",
        "        left_dataset = np.array([row for row in dataset if row[feature_idx] <= threshold])\n",
        "        right_dataset = np.array([row for row in dataset if row[feature_idx] > threshold])\n",
        "\n",
        "        return left_dataset, right_dataset\n",
        "\n",
        "    def information_gain(self, parent_y, left_y, right_y):\n",
        "        left_weight = len(left_y) / len(parent_y)\n",
        "        right_weight = len(right_y) / len(parent_y)\n",
        "\n",
        "        information_gain = self.entropy(parent_y) - (left_weight * self.entropy(left_y) + right_weight * self.entropy(right_y))\n",
        "        return information_gain\n",
        "\n",
        "    def entropy(self, y):\n",
        "        entropy = 0\n",
        "\n",
        "        class_labels = np.unique(y)\n",
        "        for class_label in class_labels:\n",
        "            p = len(y[y == class_label]) / len(y)\n",
        "            entropy += -p * np.log2(p)\n",
        "\n",
        "        return entropy\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        dataset = np.concatenate([X, y.reshape(-1, 1)], axis=1)\n",
        "        self.root = self.build_tree(dataset)\n",
        "\n",
        "    def predict(self, X):\n",
        "        predictions = [self.predict_class(row, self.root) for row in X]\n",
        "        return predictions\n",
        "\n",
        "    def predict_class(self, row, node):\n",
        "        if node.value != None:\n",
        "            return node.value\n",
        "\n",
        "        feature_val = row[node.feature_idx]\n",
        "        if feature_val <= node.threshold:\n",
        "            return self.predict_class(row, node.left)\n",
        "        else:\n",
        "            return self.predict_class(row, node.right)\n",
        "\n",
        "    def print_tree(self, node=None, depth=0, indent=\"|   \"):\n",
        "        prefix = indent * depth\n",
        "\n",
        "        if node is None:\n",
        "            node = self.root\n",
        "\n",
        "        if node.value is not None:\n",
        "            print(f\"{prefix}|--- class: {node.value}\")\n",
        "            return\n",
        "\n",
        "        feature_label = f\"Feature {node.feature_idx}\"\n",
        "\n",
        "        print(f\"{prefix}|--- {feature_label} <= {node.threshold}\")\n",
        "        self.print_tree(node.left, depth + 1, indent)\n",
        "\n",
        "# class decision tree"
      ],
      "metadata": {
        "id": "bHtifv8gwJ1D"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dt = DecisionTree(min_samples_split=2, max_depth=2)\n",
        "dt.fit(X_train, y_train)\n",
        "predictions = dt.predict(X_test)\n",
        "\n",
        "accuracy = np.mean(predictions == y_test) * 100\n",
        "print(f\"Accuracy: {accuracy:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1EEMa538wJ3D",
        "outputId": "ac6f0a18-7165-448b-c579-28b99b64cbe8"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 96.67%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dt.print_tree()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hPuy2zxBOmF_",
        "outputId": "16bcb72d-1b4c-48f6-8782-948684ce1342"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "|--- Feature 2 <= 1.9\n",
            "|   |--- class: Iris-setosa\n"
          ]
        }
      ]
    }
  ]
}